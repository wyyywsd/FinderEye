import Vision
import CoreML
import UIKit
import ImageIO
import Combine
import CoreImage
import CoreImage.CIFilterBuiltins

/// è´Ÿè´£åŠ è½½ YOLO-World Core ML æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†
final class ObjectDetectionService {
    
    // MARK: - Properties
    
    private var visionModel: VNCoreMLModel?
    private var detectionRequest: VNCoreMLRequest?
    private var cancellables = Set<AnyCancellable>()
    private var modelInputSize: CGSize = CGSize(width: 640, height: 640) // Default, will be updated from model
    
    // ç”¨äºè¿‡æ»¤ç»“æœçš„ç½®ä¿¡åº¦é˜ˆå€¼
    // private let confidenceThreshold: Float = 0.3 // å·²åºŸå¼ƒï¼Œæ”¹ç”¨ SettingsManager

    
    // æ¨¡å‹æ–‡ä»¶åç§° (Dynamic)
    // private let modelName = "ObjectDetector"
    
    // NMS IOU é˜ˆå€¼
    private let iouThreshold: Float = 0.45
    
    // è¾¹ç¼˜è¿‡æ»¤é˜ˆå€¼ (0.0 - 1.0)
    // å¦‚æœç‰©ä½“ä¸­å¿ƒç‚¹è·ç¦»è¾¹ç¼˜å°äºæ­¤å€¼ï¼Œåˆ™è§†ä¸ºè¾¹ç¼˜è¯¯æ£€
    private let edgeMargin: CGFloat = 0.02
    
    // MARK: - Slicing Logic (Stateful)
    
    // ç”¨äºåˆ†æ—¶æ£€æµ‹çš„ ROI ç´¢å¼•
    private var currentSliceIndex = 0
    
    // å®šä¹‰åˆ‡ç‰‡ ROIs (Full + 5 Slices)
    private let timeSlicedROIs: [CGRect] = [
        CGRect(x: 0, y: 0, width: 1, height: 1),         // Full (æ•´ä½“)
        CGRect(x: 0, y: 0.4, width: 0.6, height: 0.6),   // Top-Left
        CGRect(x: 0.4, y: 0.4, width: 0.6, height: 0.6), // Top-Right
        CGRect(x: 0, y: 0, width: 0.6, height: 0.6),     // Bottom-Left
        CGRect(x: 0.4, y: 0, width: 0.6, height: 0.6),   // Bottom-Right
        CGRect(x: 0.2, y: 0.2, width: 0.6, height: 0.6)  // Center
    ]
    
    /// æ‰§è¡Œåˆ†æ—¶é«˜ç²¾åº¦æ£€æµ‹
    /// æ¯æ¬¡è°ƒç”¨åªå¤„ç†ä¸€ä¸ª ROI (å…¨å›¾æˆ–æŸä¸ªåˆ‡ç‰‡)ï¼Œé€šè¿‡è½®è¯¢å®ç°å…¨è¦†ç›–
    /// è¿”å›å…ƒç»„ï¼š(æœ¬æ¬¡æ£€æµ‹ç»“æœ, æœ¬æ¬¡æ£€æµ‹çš„ ROI ç´¢å¼•)
    func detectTimeSliced(on pixelBuffer: CVPixelBuffer, searchKeyword: String, orientation: CGImagePropertyOrientation = .up) async throws -> (results: [RecognitionResult], sliceIndex: Int) {
        
        // 1. Determine ROI Index
        var targetIndex = 0
        currentSliceIndex = (currentSliceIndex + 1) % timeSlicedROIs.count
        targetIndex = currentSliceIndex
        let roiNorm = timeSlicedROIs[targetIndex]
        
        // 2. Prepare Image (Crop + Letterbox)
        // We do this manually to ensure aspect ratio is preserved via letterboxing
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer).oriented(orientation)
        let width = CVPixelBufferGetWidth(pixelBuffer)
        let height = CVPixelBufferGetHeight(pixelBuffer)
        
        return try await Task.detached(priority: .userInitiated) {
            // A. Crop
            // CIImage uses Bottom-Left origin.
            // ROI is defined in Vision coordinates (Bottom-Left origin).
            let roiY_CI = roiNorm.origin.y * CGFloat(height)
            
            let roiRect = CGRect(
                x: roiNorm.origin.x * CGFloat(width),
                y: roiY_CI,
                width: roiNorm.width * CGFloat(width),
                height: roiNorm.height * CGFloat(height)
            )
            let cropped = ciImage.cropped(to: roiRect)
            if cropped.extent.isEmpty { return ([], targetIndex) }
            
            // B. Detect
            do {
                let partResults = try await self.detectWithLetterbox(image: cropped, searchKeyword: searchKeyword, filterArtifacts: true)
                
                // C. Map
                var finalResults: [RecognitionResult] = []
                if targetIndex == 0 {
                    finalResults = partResults
                } else {
                    finalResults = partResults.map { res in
                        let box = res.boundingBox
                        let globalRect = CGRect(
                            x: roiNorm.origin.x + box.origin.x * roiNorm.width,
                            y: roiNorm.origin.y + box.origin.y * roiNorm.height,
                            width: box.width * roiNorm.width,
                            height: box.height * roiNorm.height
                        )
                        return RecognitionResult(
                            text: res.text,
                            boundingBox: globalRect,
                            confidence: res.confidence,
                            type: res.type
                        )
                    }
                }
                
                let filtered = ObjectDetectionService.filterRealTimeArtifacts(finalResults)
                return (filtered, targetIndex)
                
            } catch {
                print("Time Sliced Detection Error: \(error)")
                return ([], targetIndex)
            }
        }.value
    }
    
    // MARK: - Initialization
    
    init() {
        setupModel()
        
        // ç›‘å¬æ¨¡å‹è®¾ç½®å˜åŒ–
        SettingsManager.shared.$modelType
            .dropFirst()
            .removeDuplicates()
            .sink { [weak self] _ in
                self?.setupModel()
            }
            .store(in: &cancellables)
    }
    
    private func setupModel() {
        // Use detached task to avoid blocking the Main Thread during model loading
        Task.detached(priority: .userInitiated) { [weak self] in
            do {
                // 1. å¯»æ‰¾æ¨¡å‹æ–‡ä»¶è·¯å¾„
                // Access SettingsManager on MainActor safely if needed, or assume it's thread-safe (it is @Published but access might need care)
                // SettingsManager.shared is a class. Accessing .modelType might be on MainActor if it's an ObservableObject?
                // Actually SettingsManager.shared is likely a singleton.
                // Let's grab the model name before detaching or safely inside.
                
                let modelName = await MainActor.run { SettingsManager.shared.modelType.fileName }
                
                print("ğŸ”„ Loading model: \(modelName)...")
                
                // æ³¨æ„ï¼šåœ¨å®é™… App Bundle ä¸­ï¼Œç¼–è¯‘åçš„æ¨¡å‹é€šå¸¸æ˜¯ .mlmodelc æ–‡ä»¶å¤¹
                // è¿™é‡Œæˆ‘ä»¬å‡è®¾ .mlpackage å·²ç»è¢« Xcode ç¼–è¯‘å¹¶æ‰“åŒ…è¿› Bundle
                guard let modelURL = Bundle.main.url(forResource: modelName, withExtension: "mlmodelc") else {
                    print("âŒ Error: Could not find \(modelName).mlmodelc in bundle.")
                    return
                }
                
                // 2. åŠ è½½ Core ML æ¨¡å‹ (Heavy operation)
                let config = MLModelConfiguration()
                config.computeUnits = .all // ä¼˜å…ˆä½¿ç”¨ NPU (ANE)
                
                let model = try MLModel(contentsOf: modelURL, configuration: config)
                let visionModel = try VNCoreMLModel(for: model)
                
                // 3. Update state on MainActor or safely
                guard let self = self else { return }
                self.visionModel = visionModel
                
                // è·å–æ¨¡å‹è¾“å…¥å°ºå¯¸
                if let inputDesc = model.modelDescription.inputDescriptionsByName.first?.value,
                   let constraint = inputDesc.imageConstraint {
                    self.modelInputSize = CGSize(width: CGFloat(constraint.pixelsWide), height: CGFloat(constraint.pixelsHigh))
                    print("âœ… Model input size detected: \(self.modelInputSize)")
                }
                
                // 4. åˆ›å»º Vision è¯·æ±‚
                let request = VNCoreMLRequest(model: visionModel)
                // å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ scaleFill é…åˆæ‰‹åŠ¨ Letterbox
                // æˆ‘ä»¬å°†æ‰‹åŠ¨æŠŠå›¾ç‰‡å¡«å……ä¸ºæ­£æ–¹å½¢ (å¸¦ç°æ¡)ï¼Œæ‰€ä»¥è¿™é‡Œå‘Šè¯‰ Vision ç›´æ¥æ‹‰ä¼¸å¡«å……å³å¯ (å®é™…ä¸Šä¸ä¼šæ‹‰ä¼¸ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¤„ç†å¥½äº†é•¿å®½æ¯”)
                // è¿™æ ·å¯ä»¥å®Œå…¨æŒæ§é¢„å¤„ç†è¿‡ç¨‹ï¼Œé¿å… Vision è‡ªåŠ¨ç¼©æ”¾å¯¼è‡´çš„åæ ‡é—®é¢˜æˆ–æŒ¤å‹é—®é¢˜
                request.imageCropAndScaleOption = .scaleFill
                self.detectionRequest = request
                
                print("âœ… ObjectDetectionService initialized successfully with \(modelName).")
                
            } catch {
                print("âŒ Failed to load Core ML model: \(error)")
            }
        }
    }
    
    // MARK: - Public API
    
    /// æ‰§è¡Œç‰©å“æ£€æµ‹ (Buffer)
    func detect(on pixelBuffer: CVPixelBuffer, searchKeyword: String) async throws -> [RecognitionResult] {
        // 1. Convert to CIImage
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // 2. Perform Letterbox Detection
        return try await detectWithLetterbox(image: ciImage, searchKeyword: searchKeyword, filterArtifacts: true)
    }
    
    /// æ‰§è¡Œé«˜ç²¾åº¦ç‰©å“æ£€æµ‹ (Buffer ç‰ˆï¼Œä½¿ç”¨ ROI åˆ‡ç‰‡)
    /// é€‚ç”¨äºå®æ—¶æµï¼Œé€šè¿‡ Vision çš„ RegionOfInterest é«˜æ•ˆåˆ†å—
    func detectHighAccuracy(on pixelBuffer: CVPixelBuffer, searchKeyword: String, orientation: CGImagePropertyOrientation = .up) async throws -> [RecognitionResult] {
        // ç”±äºæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ Letterboxï¼Œä¸èƒ½ç®€å•ä½¿ç”¨ Vision çš„ ROI (å› ä¸º Vision ROI æ˜¯åœ¨åŸå›¾ä¸Šåˆ‡ï¼Œåˆ‡å®Œå Vision å†ç¼©æ”¾)
        // ä¸ºäº†ä¿è¯ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æ‰‹åŠ¨åˆ‡ç‰‡ -> Letterbox -> Detect
        
        let width = CVPixelBufferGetWidth(pixelBuffer)
        let height = CVPixelBufferGetHeight(pixelBuffer)
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer).oriented(orientation)
        
        // å®šä¹‰åˆ‡ç‰‡ ROIs (å½’ä¸€åŒ–)
        let rois = [
            CGRect(x: 0, y: 0, width: 1, height: 1),         // Full
            CGRect(x: 0, y: 0.4, width: 0.6, height: 0.6),   // Top-Left
            CGRect(x: 0.4, y: 0.4, width: 0.6, height: 0.6), // Top-Right
            CGRect(x: 0, y: 0, width: 0.6, height: 0.6),     // Bottom-Left
            CGRect(x: 0.4, y: 0, width: 0.6, height: 0.6),   // Bottom-Right
            CGRect(x: 0.2, y: 0.2, width: 0.6, height: 0.6)  // Center
        ]
        
        return try await Task.detached(priority: .userInitiated) {
            var allResults: [RecognitionResult] = []
            
            for (index, roiNorm) in rois.enumerated() {
                // A. Crop
                // CIImage cropping uses Bottom-Left coordinates
                // ROI is defined in Vision coordinates (Bottom-Left origin)
                // So we can use y directly.
                let roiY_CI = roiNorm.origin.y * CGFloat(height)
                
                let roiRect = CGRect(
                    x: roiNorm.origin.x * CGFloat(width),
                    y: roiY_CI,
                    width: roiNorm.width * CGFloat(width),
                    height: roiNorm.height * CGFloat(height)
                )
                
                let cropped = ciImage.cropped(to: roiRect)
                // å¦‚æœ crop åŒºåŸŸä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡
                if cropped.extent.isEmpty { continue }
                
                // B. Detect
                let partResults = try await self.detectWithLetterbox(image: cropped, searchKeyword: searchKeyword, filterArtifacts: true)
                
                // C. Map back to global
                if index == 0 {
                    allResults.append(contentsOf: partResults)
                } else {
                    // partResults are relative to the crop (roiNorm)
                    // Global = ROI.origin + Local * ROI.size
                    let mapped = partResults.map { res -> RecognitionResult in
                        let box = res.boundingBox
                        let globalRect = CGRect(
                            x: roiNorm.origin.x + box.origin.x * roiNorm.width,
                            y: roiNorm.origin.y + box.origin.y * roiNorm.height,
                            width: box.width * roiNorm.width,
                            height: box.height * roiNorm.height
                        )
                        return RecognitionResult(
                            text: res.text,
                            boundingBox: globalRect,
                            confidence: res.confidence,
                            type: res.type
                        )
                    }
                    allResults.append(contentsOf: mapped)
                }
            }
            
            // Filter & Merge
            let filtered = ObjectDetectionService.filterRealTimeArtifacts(allResults)
            let merged = ObjectDetectionService.mergeFragmentedDetections(filtered)
            return ObjectDetectionService.applyNMS(merged, iouThreshold: 0.45)
        }.value
    }
    
    /// æ‰§è¡Œç‰©å“æ£€æµ‹ (CGImage)
    func detect(on image: CGImage, orientation: CGImagePropertyOrientation = .up, searchKeyword: String) async throws -> [RecognitionResult] {
        let ciImage = CIImage(cgImage: image).oriented(orientation)
        return try await detectWithLetterbox(image: ciImage, searchKeyword: searchKeyword, filterArtifacts: false)
    }
    
    /// æ‰§è¡Œé«˜ç²¾åº¦ç‰©å“æ£€æµ‹ (é€‚ç”¨äºé™æ€å¤§å›¾ï¼Œä½¿ç”¨åˆ‡ç‰‡ç­–ç•¥)
    /// å°†å›¾ç‰‡åˆ‡åˆ†ä¸º 2x2 çš„ç½‘æ ¼åˆ†åˆ«æ£€æµ‹ï¼Œå¹¶ä¸å…¨å›¾æ£€æµ‹ç»“æœåˆå¹¶
    func detectHighAccuracy(on image: CGImage, orientation: CGImagePropertyOrientation = .up, searchKeyword: String) async throws -> [RecognitionResult] {
        // 0. é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–å›¾ç‰‡æ–¹å‘ä¸º .up
        // è¿™æ˜¯ä¸ºäº†ç¡®ä¿åˆ‡ç‰‡é€»è¾‘ (åŸºäº Raw Image) å’Œ Vision æ£€æµ‹é€»è¾‘ (åŸºäº Rotated Image) çš„åæ ‡ç³»ä¸€è‡´
        // å¦‚æœ Orientation ä¸æ˜¯ .upï¼ŒRaw Image çš„å·¦ä¸Šè§’å¯èƒ½å¹¶ä¸æ˜¯æ˜¾ç¤ºå›¾ç‰‡çš„å·¦ä¸Šè§’ï¼Œä¼šå¯¼è‡´åæ ‡æ˜ å°„é”™ä¹±
        var targetImage = image
        if orientation != .up {
            let uiOrientation: UIImage.Orientation
            switch orientation {
            case .up: uiOrientation = .up
            case .upMirrored: uiOrientation = .upMirrored
            case .down: uiOrientation = .down
            case .downMirrored: uiOrientation = .downMirrored
            case .left: uiOrientation = .left
            case .leftMirrored: uiOrientation = .leftMirrored
            case .right: uiOrientation = .right
            case .rightMirrored: uiOrientation = .rightMirrored
            default: uiOrientation = .up
            }
            
            let uiImage = UIImage(cgImage: image, scale: 1.0, orientation: uiOrientation)
            if let fixed = uiImage.fixedOrientation()?.cgImage {
                targetImage = fixed
            }
        }
        
        // 1. å…¨å›¾æ£€æµ‹ (æ•æ‰å¤§ç‰©ä½“å’Œæ•´ä½“ä¸Šä¸‹æ–‡)
        // æ³¨æ„ï¼šè¿™é‡Œ orientation ä¼  .upï¼Œå› ä¸º targetImage å·²ç»è¢«è½¬æ­£äº†
        let fullResults = try await detect(on: targetImage, orientation: .up, searchKeyword: searchKeyword)
        
        // å¦‚æœå›¾ç‰‡å¤ªå°ï¼Œåˆ‡ç‰‡æ²¡æœ‰æ„ä¹‰ï¼Œç›´æ¥è¿”å›
        let width = targetImage.width
        let height = targetImage.height
        if width < 1000 || height < 1000 {
            return fullResults
        }
        
        // 2. åˆ‡ç‰‡æ£€æµ‹ (2x2) + ä¸­å¿ƒé‡å å—
        var slicedResults: [RecognitionResult] = []
        let rows = 2
        let cols = 2
        
        // å¢åŠ  20% çš„é‡å ç‡ï¼Œé˜²æ­¢ç‰©ä½“æ­£å¥½åœ¨åˆ‡å‰²çº¿ä¸Šè¢«åˆ‡æ–­å¯¼è‡´æ— æ³•è¯†åˆ«
        let overlapRatio: CGFloat = 0.2
        let tileWidth = CGFloat(width) / CGFloat(cols)
        let tileHeight = CGFloat(height) / CGFloat(rows)
        
        // å®é™…åˆ‡ç‰‡å¤§å° (åŒ…å«é‡å éƒ¨åˆ†)
        let effectiveTileWidth = tileWidth * (1 + overlapRatio)
        let effectiveTileHeight = tileHeight * (1 + overlapRatio)
        
        // ä½¿ç”¨ TaskGroup å¹¶è¡Œå¤„ç†åˆ‡ç‰‡
        let tilesResults = await withTaskGroup(of: [RecognitionResult].self) { group -> [RecognitionResult] in
            // A. 2x2 ç½‘æ ¼ (å¸¦é‡å )
            for row in 0..<rows {
                for col in 0..<cols {
                    // è®¡ç®—èµ·å§‹åæ ‡
                    // é€»è¾‘ï¼šæ¯ä¸ªå—å‘å³/ä¸‹å»¶ä¼¸é‡å ï¼Œé™¤äº†æœ€åä¸€è¡Œ/åˆ—
                    // ä½†ä¸ºäº†è¦†ç›–ä¸­é—´çš„ç¼éš™ï¼Œç®€å•çš„åšæ³•æ˜¯è®©æ¯ä¸ªå—éƒ½æ¯”æ ‡å‡†å¤§ 20%ï¼Œå¹¶ä¿æŒä¸­å¿ƒç‚¹æˆ–è€… TopLeft é€‚å½“åç§»
                    // é‡‡ç”¨æœ€ç¨³å¥çš„ç­–ç•¥ï¼šåŸºäºæ ‡å‡†ç½‘æ ¼ä¸­å¿ƒç‚¹å‘å¤–æ‰©å¼ 
                    
                    // æ ‡å‡†ç½‘æ ¼èµ·å§‹ç‚¹
                    let originX = CGFloat(col) * tileWidth
                    let originY = CGFloat(row) * tileHeight
                    
                    // è°ƒæ•´èµ·å§‹ç‚¹ä»¥å®ç°å±…ä¸­æ‰©å¼  (Centered Expansion)
                    // newX = centerX - newWidth / 2
                    // centerX = originX + tileWidth / 2
                    // newX = originX + tileWidth / 2 - tileWidth * (1 + overlap) / 2
                    //      = originX - tileWidth * overlap / 2
                    
                    let centerX = originX + tileWidth / 2
                    let centerY = originY + tileHeight / 2
                    
                    var x = centerX - effectiveTileWidth / 2
                    var y = centerY - effectiveTileHeight / 2
                    
                    // è¾¹ç•Œä¿®æ­£ï¼šä¸èƒ½è¶…å‡ºå›¾åƒèŒƒå›´ too much (Vision handle crop ok?)
                    // CGImage cropping handles out of bounds by returning null or empty usually? 
                    // No, we must ensure rect is within bounds.
                    // But if we clamp, we lose the overlap at the edges? 
                    // Actually, at the edges (0 and width), we don't need overlap OUTSIDE.
                    // We only need overlap INSIDE.
                    
                    // ä¿®æ­£ç­–ç•¥ï¼š
                    // Col 0: x = 0, width = tileWidth + overlapAmount
                    // Col 1: x = width - (tileWidth + overlapAmount), width = ...
                    
                    // å¯¹äº 2x2 è¿™ç§ç®€å•æƒ…å†µï¼Œç›´æ¥ç¡¬ç¼–ç æœ€å®‰å…¨
                    if col == 0 { x = 0 }
                    else { x = CGFloat(width) - effectiveTileWidth }
                    
                    if row == 0 { y = 0 }
                    else { y = CGFloat(height) - effectiveTileHeight }
                    
                    // ç¡®ä¿ rect åˆæ³• (é˜²æ­¢ effective > full)
                    let w = min(effectiveTileWidth, CGFloat(width))
                    let h = min(effectiveTileHeight, CGFloat(height))
                    x = max(0, min(x, CGFloat(width) - w))
                    y = max(0, min(y, CGFloat(height) - h))
                    
                    let rect = CGRect(x: x, y: y, width: w, height: h)
                    self.addTask(to: &group, rect: rect, image: targetImage, orientation: .up, searchKeyword: searchKeyword, fullSize: CGSize(width: width, height: height))
                }
            }
            
            // B. ä¸­å¿ƒé‡å å— (è§£å†³åå­—ç›²åŒº) - ä¾ç„¶ä¿ç•™ï¼Œä½œä¸ºåŒé‡ä¿é™©
            let centerRect = CGRect(x: CGFloat(width) / 4, y: CGFloat(height) / 4, width: CGFloat(width) / 2, height: CGFloat(height) / 2)
            self.addTask(to: &group, rect: centerRect, image: targetImage, orientation: .up, searchKeyword: searchKeyword, fullSize: CGSize(width: width, height: height))
            
            var all = [RecognitionResult]()
            for await res in group {
                all.append(contentsOf: res)
            }
            return all
        }
        
        slicedResults.append(contentsOf: tilesResults)
        
        // 3. åˆå¹¶å…¨å›¾ç»“æœå’Œåˆ‡ç‰‡ç»“æœï¼Œå¹¶è¿è¡Œ NMS å»é‡
        let combinedResults = fullResults + slicedResults
        
        // 3.5 ç¢ç‰‡åˆå¹¶ (æ–°å¢ï¼šè§£å†³åˆ†å—å¯¼è‡´çš„ç‰©ä½“åˆ‡æ–­é—®é¢˜)
        // æ³¨æ„ï¼šé™æ€æ£€æµ‹é€šå¸¸ä¸éœ€è¦ filterRealTimeArtifactsï¼Œä½†éœ€è¦ç¢ç‰‡åˆå¹¶
        let mergedResults = ObjectDetectionService.mergeFragmentedDetections(combinedResults)
        
        return ObjectDetectionService.applyNMS(mergedResults, iouThreshold: 0.45)
    }
    
    private func addTask(to group: inout TaskGroup<[RecognitionResult]>, rect: CGRect, image: CGImage, orientation: CGImagePropertyOrientation, searchKeyword: String, fullSize: CGSize) {
        guard let cropped = image.cropping(to: rect) else { return }
        
        group.addTask {
            do {
                // åˆ‡ç‰‡æ£€æµ‹ä¹Ÿå¿…é¡»å…³é—­ä¼ªå½±è¿‡æ»¤ï¼Œå¦åˆ™åˆ‡ç‰‡è¾¹ç¼˜ç‰©ä½“ä¼šè¢«è¯¯åˆ 
                let results = try await self.detect(on: cropped, orientation: orientation, searchKeyword: searchKeyword)
                
                // åæ ‡æ˜ å°„
                let offsetX = rect.minX
                let offsetY = rect.minY // CGImage Y starts from top usually, but let's check mapping
                
                // Wait, in previous logic I used scale ratios. Here let's use absolute logic then normalize.
                // Or better, keep using relative logic if possible, but rect is absolute here.
                
                // Vision returns normalized [0,1] relative to the *cropped* image.
                // We need to convert to [0,1] relative to *full* image.
                
                // Note: Vision coordinates (Y up) vs CGImage (Y down usually, but depends on context).
                // However, detect() returns normalized coordinates (0-1).
                // So we just need to scale and translate the normalized box.
                
                // Let's assume standard normalized Vision coordinates (0,0 is bottom-left).
                // If we cropped the Top-Left of the image (CGImage coordinates):
                // That corresponds to Top-Left in Vision too if orientation is handled.
                // Actually Vision handles orientation.
                
                // Let's rely on the relative position of the crop rect in the full image.
                // Crop Rect (CGImage) -> Normalized Rect in Full Image
                // Note: CGImage origin is Top-Left. Vision origin is Bottom-Left.
                // This coordinate flip is tricky.
                
                // Let's simplify:
                // If we crop a rect from CGImage, Vision sees that crop as a full image [0,0,1,1].
                // We need to map that back to the full image's normalized space.
                
                // Calculate crop's normalized frame in full image
                let normX = rect.minX / fullSize.width
                let normW = rect.width / fullSize.width
                // For Y: CGImage (0 at top) vs Vision (0 at bottom)
                // If we crop top-left (y=0 in CGImage), that is y=1 in Vision space?
                // Actually, let's look at how `detect` handles it.
                // `detect` passes `orientation`.
                // If we pass the same orientation for the crop, Vision handles it.
                // The issue is mapping the result box back.
                
                // Safe bet: Assume Vision works in "Image Space" regardless of orientation tag?
                // No, orientation tag rotates the image before processing.
                
                // Let's look at the previous implementation's logic:
                // "Vision Y è½´æ˜¯ä»åº•éƒ¨å¼€å§‹çš„... row 0 (CGImage Top) å¯¹åº” Vision çš„é«˜ä½ Y"
                // This suggests we need to flip Y if we use CGImage crop logic.
                
                let normY_CG = rect.minY / fullSize.height
                let normH = rect.height / fullSize.height
                
                // Convert CGImage crop rect (Top-Left origin) to Vision crop rect (Bottom-Left origin)
                // Vision Y = 1 - CGImage Y - Height
                let normY_Vision = 1.0 - normY_CG - normH
                
                return results.map { res in
                    let box = res.boundingBox
                    let newRect = CGRect(
                        x: normX + box.origin.x * normW,
                        y: normY_Vision + box.origin.y * normH,
                        width: box.width * normW,
                        height: box.height * normH
                    )
                    
                    return RecognitionResult(
                        text: res.text,
                        boundingBox: newRect,
                        confidence: res.confidence,
                        type: res.type
                    )
                }
            } catch {
                return []
            }
        }
    }
    
    // Removed legacy detect(with handler...) method
    // MARK: - Letterbox Helper
    
    private struct LetterboxInfo {
        let image: CIImage
        let scale: CGFloat
        let offset: CGPoint
        let originalSize: CGSize
        let newSize: CGSize
    }
    
    /// å°†å›¾ç‰‡è¿›è¡Œ Letterbox å¤„ç† (ä¿æŒé•¿å®½æ¯”ç¼©æ”¾åˆ°ç›®æ ‡æ­£æ–¹å½¢å°ºå¯¸ï¼Œå¹¶å¡«å……ç°è‰²èƒŒæ™¯)
    /// è§£å†³ 16:9 å›¾ç‰‡è¢«å¼ºè¡Œå‹ç¼©åˆ° 1:1 æ¨¡å‹è¾“å…¥å¯¼è‡´çš„å½¢å˜é—®é¢˜
    private func letterbox(image: CIImage, targetSize: CGSize) -> LetterboxInfo {
        let originalSize = image.extent.size
        // è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ (Fit)
        let scale = min(targetSize.width / originalSize.width, targetSize.height / originalSize.height)
        
        let newWidth = originalSize.width * scale
        let newHeight = originalSize.height * scale
        
        // å±…ä¸­åç§»
        let offsetX = (targetSize.width - newWidth) / 2.0
        let offsetY = (targetSize.height - newHeight) / 2.0
        
        // 1. ç¼©æ”¾ + å¹³ç§»
        // æ³¨æ„ï¼šCIImage å˜æ¢åŸç‚¹
        let scaledImage = image
            .transformed(by: CGAffineTransform(scaleX: scale, y: scale))
            .transformed(by: CGAffineTransform(translationX: offsetX, y: offsetY))
        
        // 2. èƒŒæ™¯ (YOLO å¸¸ç”¨ç°è‰² 114/255)
        let background = CIImage(color: CIColor(red: 114/255, green: 114/255, blue: 114/255))
            .cropped(to: CGRect(origin: .zero, size: targetSize))
        
        // 3. åˆæˆ (SourceOver)
        let resultImage = scaledImage.composited(over: background)
        
        return LetterboxInfo(
            image: resultImage,
            scale: scale,
            offset: CGPoint(x: offsetX, y: offsetY),
            originalSize: originalSize,
            newSize: CGSize(width: newWidth, height: newHeight)
        )
    }
    
    /// æ ¸å¿ƒæ£€æµ‹é€»è¾‘ (ä½¿ç”¨ Letterbox)
    private func detectWithLetterbox(image: CIImage, searchKeyword: String, filterArtifacts: Bool) async throws -> [RecognitionResult] {
        guard let request = detectionRequest else {
            setupModel()
            return []
        }
        
        let targetSize = self.modelInputSize
        let info = letterbox(image: image, targetSize: targetSize)
        
        let handler = VNImageRequestHandler(ciImage: info.image, orientation: .up) // Image already oriented
        
        let currentThreshold = await MainActor.run { Float(SettingsManager.shared.confidenceThreshold) }
        
        do {
            try handler.perform([request])
            
            guard let observations = request.results as? [VNRecognizedObjectObservation] else {
                return []
            }
            
            // Map observations back to original image space
            let results = observations.compactMap { observation -> RecognitionResult? in
                guard let topLabel = observation.labels.first else { return nil }
                if topLabel.confidence < currentThreshold { return nil }
                
                // å…³é”®è¯åŒ¹é…
                var isMatch = true
                if !searchKeyword.isEmpty {
                    let normalizedKeyword = searchKeyword.lowercased()
                    let label = topLabel.identifier.lowercased()
                    isMatch = label.contains(normalizedKeyword) || normalizedKeyword.contains(label)
                    if !isMatch {
                        if let english = ObjectTranslation.getEnglishName(for: searchKeyword)?.lowercased() {
                            isMatch = label.contains(english) || english.contains(label)
                        }
                    }
                }
                
                if isMatch {
                    // Coordinate Mapping
                    // Vision Box (0-1 in Target Square) -> Pixel in Target
                    let box = observation.boundingBox
                    
                    // Vision origin is Bottom-Left
                    // CIImage origin is Bottom-Left
                    // Perfect match.
                    
                    let x_pixel_target = box.origin.x * targetSize.width
                    let y_pixel_target = box.origin.y * targetSize.height
                    let w_pixel_target = box.width * targetSize.width
                    let h_pixel_target = box.height * targetSize.height
                    
                    // Remove Padding
                    let x_pixel_new = x_pixel_target - info.offset.x
                    let y_pixel_new = y_pixel_target - info.offset.y
                    
                    // Un-scale
                    let x_pixel_orig = x_pixel_new / info.scale
                    let y_pixel_orig = y_pixel_new / info.scale
                    let w_pixel_orig = w_pixel_target / info.scale
                    let h_pixel_orig = h_pixel_target / info.scale
                    
                    // Normalize to Original Size
                    let normRect = CGRect(
                        x: x_pixel_orig / info.originalSize.width,
                        y: y_pixel_orig / info.originalSize.height,
                        width: w_pixel_orig / info.originalSize.width,
                        height: h_pixel_orig / info.originalSize.height
                    )
                    
                    return RecognitionResult(
                        text: topLabel.identifier,
                        boundingBox: normRect,
                        confidence: topLabel.confidence,
                        type: .object
                    )
                }
                return nil
            }
            
            var finalResults = results
            if filterArtifacts {
                finalResults = ObjectDetectionService.filterRealTimeArtifacts(results)
            }
            
            return ObjectDetectionService.applyNMS(finalResults, iouThreshold: 0.45)
            
        } catch {
            throw error
        }
    }

    // MARK: - Private Helpers
    
    /// å°è¯•åˆå¹¶ç›¸é‚»çš„ç¢ç‰‡æ£€æµ‹ç»“æœ (åœ¨ NMS ä¹‹å‰è°ƒç”¨)
    /// è§£å†³åˆ†å—æ£€æµ‹å¯¼è‡´çš„ç‰©ä½“è¢«åˆ‡æ–­é—®é¢˜ (ä¸é‡å ä½†ç©ºé—´ç›¸é‚»)
    private static func mergeFragmentedDetections(_ results: [RecognitionResult]) -> [RecognitionResult] {
        if results.isEmpty { return [] }
        
        // å…³é”®ä¿®æ”¹ï¼šä¸å†æŒ‰æ ‡ç­¾ä¸¥æ ¼åˆ†ç»„ï¼Œå…è®¸ä¸åŒæ ‡ç­¾ä½†ç©ºé—´é‡å çš„ç‰©ä½“åˆå¹¶ (ä¾‹å¦‚ "mouse" å’Œ "black mouse")
        // å‰ææ˜¯è¿™äº›ç»“æœéƒ½å·²ç»é€šè¿‡äº†å…³é”®è¯è¿‡æ»¤ï¼Œè¯´æ˜å®ƒä»¬éƒ½ä¸ç”¨æˆ·æœç´¢ç›¸å…³
        
        var sortedItems = results.sorted { $0.boundingBox.minX < $1.boundingBox.minX }
        var mergedGroups: [[RecognitionResult]] = []
        
        while !sortedItems.isEmpty {
            var currentGroup = [sortedItems.removeFirst()]
            var changed = true
            
            while changed {
                changed = false
                var nextRoundItems: [RecognitionResult] = []
                
                for item in sortedItems {
                    // æ£€æŸ¥ item æ˜¯å¦ä¸ currentGroup ä¸­çš„ä»»æ„ä¸€ä¸ªè¶³å¤Ÿ"æ¥è¿‘"
                    if isCloseEnough(item, to: currentGroup) {
                        currentGroup.append(item)
                        changed = true
                    } else {
                        nextRoundItems.append(item)
                    }
                }
                sortedItems = nextRoundItems
            }
            mergedGroups.append(currentGroup)
        }
        
        var finalResults: [RecognitionResult] = []
        
        // å°†æ¯ç»„åˆå¹¶æˆä¸€ä¸ªå¤§æ¡†
        for group in mergedGroups {
            if group.count == 1 {
                finalResults.append(group[0])
            } else {
                // åˆå¹¶ BoundingBox
                let minX = group.map { $0.boundingBox.minX }.min() ?? 0
                let minY = group.map { $0.boundingBox.minY }.min() ?? 0
                let maxX = group.map { $0.boundingBox.maxX }.max() ?? 0
                let maxY = group.map { $0.boundingBox.maxY }.max() ?? 0
                
                let mergedRect = CGRect(x: minX, y: minY, width: maxX - minX, height: maxY - minY)
                
                // å–æœ€é«˜ç½®ä¿¡åº¦çš„ç»“æœä½œä¸ºä»£è¡¨
                // ä¼˜å…ˆé€‰æ‹©æ ‡ç­¾æ›´è¯¦ç»†çš„? æˆ–è€…ç½®ä¿¡åº¦æœ€é«˜çš„?
                // é€šå¸¸ç½®ä¿¡åº¦é«˜æ„å‘³ç€åŒ¹é…æ›´å¥½
                if let bestItem = group.max(by: { $0.confidence < $1.confidence }) {
                    finalResults.append(RecognitionResult(
                        text: bestItem.text,
                        boundingBox: mergedRect,
                        confidence: bestItem.confidence,
                        type: bestItem.type
                    ))
                }
            }
        }
        
        return finalResults
    }
    
    /// åˆ¤æ–­ä¸€ä¸ªç‰©ä½“æ˜¯å¦ä¸ä¸€ç»„ç‰©ä½“ä¸­çš„ä»»æ„ä¸€ä¸ªè¶³å¤Ÿæ¥è¿‘
    private static func isCloseEnough(_ item: RecognitionResult, to group: [RecognitionResult]) -> Bool {
        let box = item.boundingBox
        // é˜ˆå€¼ï¼šæ¡†å®½åº¦çš„ 20% æˆ– é«˜åº¦çš„ 20% (è§†ä½œç›¸é‚»)
        // æˆ–è€…å›ºå®šè·ç¦»ï¼Œä½†åœ¨å½’ä¸€åŒ–åæ ‡ä¸‹ä¸å¥½å®šã€‚ä½¿ç”¨ç›¸å¯¹å°ºå¯¸è¾ƒå¥½ã€‚
        
        for groupItem in group {
            let gBox = groupItem.boundingBox
            
            // 1. æ£€æŸ¥æ˜¯å¦æœ‰é‡å  (Intersection)
            if box.intersects(gBox) { return true }
            
            // 2. æ£€æŸ¥è·ç¦» (Distance)
            // è®¡ç®—ä¸¤ä¸ªçŸ©å½¢çš„æœ€è¿‘è·ç¦»
            let xDist = max(0, max(box.minX - gBox.maxX, gBox.minX - box.maxX))
            let yDist = max(0, max(box.minY - gBox.maxY, gBox.minY - box.maxY))
            
            // å…è®¸çš„é—´éš™ï¼šå–ä¸¤ä¸ªæ¡†ä¸­è¾ƒå°å°ºå¯¸çš„ 10%
            let toleranceX = min(box.width, gBox.width) * 0.1
            let toleranceY = min(box.height, gBox.height) * 0.1
            
            // å¦‚æœåœ¨ X æˆ– Y æ–¹å‘ä¸Šéå¸¸æ¥è¿‘ï¼Œä¸”å¦ä¸€ä¸ªæ–¹å‘ä¸Šæœ‰æ˜¾è‘—æŠ•å½±é‡å 
            // (ä¾‹å¦‚ï¼šå·¦å³ç›¸é‚»ï¼Œä¸”é«˜åº¦ä¸Šæœ‰é‡å )
            
            // æ°´å¹³ç›¸é‚» check
            let yOverlap = max(0, min(box.maxY, gBox.maxY) - max(box.minY, gBox.minY))
            let hasVerticalOverlap = yOverlap > min(box.height, gBox.height) * 0.5
            if xDist < toleranceX && hasVerticalOverlap { return true }
            
            // å‚ç›´ç›¸é‚» check
            let xOverlap = max(0, min(box.maxX, gBox.maxX) - max(box.minX, gBox.minX))
            let hasHorizontalOverlap = xOverlap > min(box.width, gBox.width) * 0.5
            if yDist < toleranceY && hasHorizontalOverlap { return true }
        }
        
        return false
    }

    // Removed legacy processObservations method

    /// è¿‡æ»¤å®æ—¶æµä¸­çš„ä¼ªå½± (è¾¹ç¼˜è¯¯æ£€ + è¿‡å°ç‰©ä½“)
    private static func filterRealTimeArtifacts(_ results: [RecognitionResult]) -> [RecognitionResult] {
        let edgeMargin: CGFloat = 0.01 // è¾¹ç¼˜ä¿ç•™åŒº 1% (Optimization: Reduced from 2%)
        let minSize: CGFloat = 0.01    // æœ€å°å°ºå¯¸ 1% (Optimization: Reduced from 3% to detect smaller objects)
        
        return results.filter { result in
            let box = result.boundingBox
            let center = CGPoint(x: box.midX, y: box.midY)
            
            // 1. è¾¹ç¼˜è¿‡æ»¤ï¼šæ£€æŸ¥ä¸­å¿ƒç‚¹æ˜¯å¦è¿‡äºé è¿‘è¾¹ç¼˜
            // å¾ˆå¤šæ—¶å€™è¾¹ç¼˜çš„â€œåŠä¸ªç‰©ä½“â€ä¼šè¢«è¯†åˆ«æˆé”™è¯¯çš„ç±»åˆ«ï¼Œæˆ–è€…æ ¹æœ¬ä¸å­˜åœ¨
            if center.x < edgeMargin || center.x > (1.0 - edgeMargin) ||
               center.y < edgeMargin || center.y > (1.0 - edgeMargin) {
                return false
            }
            
            // 2. å°ºå¯¸è¿‡æ»¤ï¼šè¿‡æ»¤æå°çš„é—ªçƒå™ªç‚¹
            // åœ¨å®æ—¶æµä¸­ï¼Œå¤ªå°çš„ç‰©ä½“é€šå¸¸ä¸ç¨³å®š
            if box.width < minSize || box.height < minSize {
                return false
            }
            
            return true
        }
    }
    
    /// éæå¤§å€¼æŠ‘åˆ¶ (NMS)
    private static func applyNMS(_ results: [RecognitionResult], iouThreshold: Float) -> [RecognitionResult] {
        // 0. åŒ…å«å…³ç³»è¿‡æ»¤ (Containment Filtering)
        // è§£å†³åˆ†å—æ£€æµ‹å¯¼è‡´çš„"å¤§æ¡†åŒ…å°æ¡†"é—®é¢˜ (ä¾‹å¦‚: å…¨å›¾æ£€æµ‹å‡ºä¸€ä¸ªå®Œæ•´é”®ç›˜ï¼Œåˆ‡ç‰‡æ£€æµ‹å‡ºåŠä¸ªé”®ç›˜)
        // å¦‚æœä¸¤ä¸ªæ¡†ç±»åˆ«ç›¸åŒï¼Œä¸”ä¸€ä¸ªæ¡†åŒ…å«äº†å¦ä¸€ä¸ªæ¡†çš„å¤§éƒ¨åˆ†åŒºåŸŸï¼Œåˆ™ä¿ç•™ç½®ä¿¡åº¦é«˜çš„é‚£ä¸ª(é€šå¸¸æ˜¯å¤§æ¡†ï¼Œæˆ–è€…ç½®ä¿¡åº¦æ›´é«˜çš„å°æ¡†)
        // ä½†ä¸ºäº†ç®€å•æœ‰æ•ˆï¼Œæˆ‘ä»¬å‡è®¾å¤§æ¡†æ˜¯æ›´å¥½çš„ç»“æœ(å› ä¸ºå®ƒå®Œæ•´)ï¼Œæˆ–è€…ç½®ä¿¡åº¦é«˜çš„æ›´å¥½ã€‚
        // è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨"åŸºäºç½®ä¿¡åº¦æ’åºåçš„ IOU æŠ‘åˆ¶"ï¼Œä½†åŠ ä¸Š"åŒ…å«æŠ‘åˆ¶"ã€‚
        
        // 1. æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
        let sortedResults = results.sorted { $0.confidence > $1.confidence }
        var selectedResults: [RecognitionResult] = []
        var activeResults = sortedResults
        
        // è°ƒæ•´ IOU é˜ˆå€¼ï¼šå¯¹äºåˆ†å—åˆå¹¶ï¼Œç¨å¾®æ¿€è¿›ä¸€ç‚¹ (0.35)
        // åŸå§‹ 0.45 å¯èƒ½å¯¼è‡´é‡å è¾ƒå¤šçš„ä¸¤ä¸ªéƒ¨åˆ†è¢«ä¿ç•™
        let effectiveIOU = min(iouThreshold, 0.35)
        
        while !activeResults.isEmpty {
            // å–å‡ºå½“å‰æœ€é«˜ç½®ä¿¡åº¦çš„æ¡†
            let best = activeResults.removeFirst()
            selectedResults.append(best)
            
            // è¿‡æ»¤æ‰ä¸ best å†²çªçš„æ¡†
            activeResults = activeResults.filter { other in
                // A. è®¡ç®— IOU
                let iou = calculateIOU(best.boundingBox, other.boundingBox)
                if iou >= CGFloat(effectiveIOU) {
                    return false // IOU è¿‡å¤§ï¼ŒæŠ‘åˆ¶
                }
                
                // B. è®¡ç®—åŒ…å«å…³ç³» (Intersection over Smaller Area)
                let intersection = best.boundingBox.intersection(other.boundingBox)
                if !intersection.isNull {
                    let intersectionArea = intersection.width * intersection.height
                    let otherArea = other.boundingBox.width * other.boundingBox.height
                    let bestArea = best.boundingBox.width * best.boundingBox.height
                    
                    // 1. å°æ¡†æŠ‘åˆ¶ (æŠ‘åˆ¶è¢«å¤§æ¡†åŒ…å«çš„å°æ¡†)
                    // å¦‚æœ other (è¾ƒå°ç½®ä¿¡åº¦) è¢« best åŒ…å« (>60%)ï¼Œä¸” best é¢ç§¯æ¯” other å¤§ -> æŠ‘åˆ¶ other
                    // é€»è¾‘ï¼šå¦‚æœæœ‰ä¸€ä¸ªé«˜ç½®ä¿¡åº¦çš„å¤§æ¡†ï¼Œé‡Œé¢çš„å°æ¡†å¤§æ¦‚ç‡æ˜¯åˆ†å—äº§ç”Ÿçš„ç¢ç‰‡
                    if intersectionArea / otherArea > 0.6 && bestArea > otherArea {
                         return false
                    }
                    
                    // 2. å¤§æ¡†æŠ‘åˆ¶ (æŠ‘åˆ¶åŒ…å«äº†å°æ¡†ä½†ç½®ä¿¡åº¦è¾ƒä½çš„å¤§æ¡† - æ…ç”¨ï¼Œé™¤éé‡å åº¦æé«˜)
                    // å¦‚æœ best è¢« other åŒ…å« (>80%)ï¼Œè¯´æ˜ best æ˜¯å±€éƒ¨çš„ç²¾ç»†æ£€æµ‹ï¼Œother æ˜¯æ•´ä½“ä½†ç½®ä¿¡åº¦ä½
                    // è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›ä¿ç•™ best (å› ä¸ºç½®ä¿¡åº¦é«˜)ï¼ŒæŠ‘åˆ¶ other (å¤§è€Œæ— å½“)
                    // è¿™é‡Œçš„é€»è¾‘å·²ç»é€šè¿‡ removeFirst å®ç°äº† (ä¿ç•™ best)ï¼Œæ‰€ä»¥åªéœ€è¦å†³å®šæ˜¯å¦æŠ‘åˆ¶ other
                    // å¦‚æœ other åŒ…å«äº† bestï¼Œä¸”é‡å åŒºåŸŸå  best çš„ 80% ä»¥ä¸Š...
                    // è¿™ç§æƒ…å†µä¸‹ IOU å¯èƒ½ä¸å¤§ (å› ä¸º other å¾ˆå¤§)ï¼Œä½†æˆ‘ä»¬ä¸åº”è¯¥æŠ‘åˆ¶ other å—ï¼Ÿ
                    // å¦‚æœ best æ˜¯é”®ç›˜çš„ä¸€éƒ¨åˆ†ï¼Œother æ˜¯æ•´ä¸ªé”®ç›˜...
                    // ç”¨æˆ·æŠ±æ€¨"è¯†åˆ«æˆå¤šä¸ª"ï¼Œæ„å‘³ç€å¤§æ¡†å’Œå°æ¡†å¹¶å­˜ã€‚
                    // æˆ‘ä»¬åº”è¯¥åªä¿ç•™ä¸€ä¸ªã€‚
                    // ç­–ç•¥ï¼šåªè¦æœ‰æ˜¾è‘—é‡å  (>60% of minArea)ï¼Œå°±åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„é‚£ä¸ªã€‚
                    let minArea = min(otherArea, bestArea)
                    if intersectionArea / minArea > 0.6 {
                         return false
                    }
                }
                
                return true
            }
        }
        
        return selectedResults
    }
    
    /// è®¡ç®—ä¸¤ä¸ªçŸ©å½¢çš„äº¤å¹¶æ¯” (IOU)
    private static func calculateIOU(_ rect1: CGRect, _ rect2: CGRect) -> CGFloat {
        let intersection = rect1.intersection(rect2)
        if intersection.isNull { return 0 }
        
        let intersectionArea = intersection.width * intersection.height
        let unionArea = rect1.width * rect1.height + rect2.width * rect2.height - intersectionArea
        
        if unionArea <= 0 { return 0 }
        return intersectionArea / unionArea
    }
}
