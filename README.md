<div align="center">

# ğŸ” FinderEye

**Transform your camera into a powerful search engine for the physical world.**

[![Platform](https://img.shields.io/badge/Platform-iOS%2017.0+-blue?style=flat-square&logo=apple)](https://developer.apple.com/ios/)
[![Swift](https://img.shields.io/badge/Swift-5.9+-F05138?style=flat-square&logo=swift&logoColor=white)](https://swift.org/)
[![SwiftUI](https://img.shields.io/badge/UI-SwiftUI-0071E3?style=flat-square&logo=swift&logoColor=white)](https://developer.apple.com/xcode/swiftui/)
[![Core ML](https://img.shields.io/badge/ML-Core%20ML-34C759?style=flat-square&logo=apple&logoColor=white)](https://developer.apple.com/machine-learning/core-ml/)
[![YOLO-World](https://img.shields.io/badge/Model-YOLO--World-FF6F00?style=flat-square)](https://github.com/AILab-CVC/YOLO-World)
[![License](https://img.shields.io/badge/License-All%20Rights%20Reserved-lightgrey?style=flat-square)](#-license)

[English](docs/README_EN.md) | [ä¸­æ–‡](docs/README_CN.md)

<br/>

<!-- Add your app screenshots here -->
<!-- <img src="docs/assets/demo.gif" width="280" alt="FinderEye Demo"> -->

</div>

---

## ğŸ“– About

**FinderEye** is an intelligent iOS application that combines real-time **Optical Character Recognition (OCR)** with offline **Open-Vocabulary Object Detection**, helping users instantly find text and objects through their camera.

Built with a minimalist aesthetic and a **privacy-first** approach, all processing runs entirely on-device â€” fast, secure, and fully functional without an internet connection.

> **FinderEye** æ˜¯ä¸€æ¬¾æ™ºèƒ½ iOS åº”ç”¨ï¼Œç»“åˆå®æ—¶ **OCR** ä¸ç¦»çº¿**å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹**ï¼Œå°†ç›¸æœºå˜ä¸ºç‰©ç†ä¸–ç•Œçš„æœç´¢å¼•æ“ã€‚æ‰€æœ‰å¤„ç†å‡åœ¨è®¾å¤‡ç«¯å®Œæˆï¼Œéšç§è‡³ä¸Šã€‚

---

## âœ¨ Features

| Feature | Description |
|:--------|:------------|
| **ğŸ” Real-time Search** | Instantly detect and highlight text & objects in the camera view |
| **ğŸ“ Text Extraction** | Extract full text from photos/camera with layout preservation |
| **ğŸ“ Perspective Crop** | Correct skewed documents using 4-corner perspective transformation |
| **ğŸ¤– Offline AI** | Powered by custom **YOLO-World** model running on **Core ML** |
| **ğŸŒ Smart Translation** | Natural language queries in Chinese (e.g., `çº¢è‰²çš„æ¯å­` â†’ `red cup`) |
| **ğŸ¨ Minimalist UI** | Distraction-free interface with haptic feedback and fluid animations |
| **ğŸ”’ Privacy First** | Zero network requests â€” all processing stays on your device |

---

## ğŸ›  Tech Stack

<table>
<tr>
<td><b>Category</b></td>
<td><b>Technology</b></td>
</tr>
<tr>
<td><b>Platform</b></td>
<td><img src="https://img.shields.io/badge/iOS-17.0+-000000?style=flat-square&logo=apple&logoColor=white" alt="iOS 17.0+"></td>
</tr>
<tr>
<td><b>Language</b></td>
<td><img src="https://img.shields.io/badge/Swift-5.9+-F05138?style=flat-square&logo=swift&logoColor=white" alt="Swift 5.9+"></td>
</tr>
<tr>
<td><b>UI Framework</b></td>
<td><img src="https://img.shields.io/badge/SwiftUI-Declarative%20UI-0071E3?style=flat-square&logo=swift&logoColor=white" alt="SwiftUI"></td>
</tr>
<tr>
<td><b>Reactive</b></td>
<td><img src="https://img.shields.io/badge/Combine-Data%20Flow-8E44AD?style=flat-square&logo=apple&logoColor=white" alt="Combine"></td>
</tr>
<tr>
<td><b>Camera</b></td>
<td><img src="https://img.shields.io/badge/AVFoundation-Camera%20Capture-FF9500?style=flat-square&logo=apple&logoColor=white" alt="AVFoundation"></td>
</tr>
<tr>
<td><b>OCR</b></td>
<td><img src="https://img.shields.io/badge/Vision-Text%20Recognition-5856D6?style=flat-square&logo=apple&logoColor=white" alt="Vision"></td>
</tr>
<tr>
<td><b>Image Processing</b></td>
<td><img src="https://img.shields.io/badge/Core%20Image-Perspective%20Correction-30B0C7?style=flat-square&logo=apple&logoColor=white" alt="Core Image"></td>
</tr>
<tr>
<td><b>ML Inference</b></td>
<td><img src="https://img.shields.io/badge/Core%20ML-On--Device%20ML-34C759?style=flat-square&logo=apple&logoColor=white" alt="Core ML"></td>
</tr>
<tr>
<td><b>ML Model</b></td>
<td><img src="https://img.shields.io/badge/YOLO--World-Open%20Vocabulary%20Detection-FF6F00?style=flat-square" alt="YOLO-World"></td>
</tr>
<tr>
<td><b>Model Export</b></td>
<td><img src="https://img.shields.io/badge/Python-Ultralytics%20%7C%20CoreMLTools-3776AB?style=flat-square&logo=python&logoColor=white" alt="Python"></td>
</tr>
</table>

---

## ğŸ— Architecture

The project follows **MVVM** architecture with a clean modular structure:

```
FinderEye/
â”œâ”€â”€ FinderEye/                    # App Source Code
â”‚   â”œâ”€â”€ Sources/
â”‚   â”‚   â”œâ”€â”€ App/                  # App Entry Point (FinderEyeApp.swift)
â”‚   â”‚   â”œâ”€â”€ Core/                 # Core Services & Utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ Camera/           #   â””â”€ Camera Management (AVFoundation)
â”‚   â”‚   â”‚   â”œâ”€â”€ Vision/           #   â””â”€ OCR Service (Vision Framework)
â”‚   â”‚   â”‚   â””â”€â”€ Utils/            #   â””â”€ Geometry, Translation, Settings
â”‚   â”‚   â”œâ”€â”€ Features/             # Feature Modules
â”‚   â”‚   â”‚   â””â”€â”€ Home/             #   â””â”€ Main Camera Interface (MVVM)
â”‚   â”‚   â”‚       â”œâ”€â”€ ViewModels/   #       â””â”€ Detection & OCR ViewModels
â”‚   â”‚   â”‚       â””â”€â”€ Views/        #       â””â”€ SwiftUI Views & Overlays
â”‚   â”‚   â””â”€â”€ Models/               # Data Models & Resources
â”‚   â”‚       â””â”€â”€ Resources/        #   â””â”€ Core ML Models (.mlpackage)
â”‚   â””â”€â”€ App-Info.plist            # App Configuration
â”œâ”€â”€ Scripts/                      # Python Scripts for Model Export
â”œâ”€â”€ docs/                         # Documentation (EN / CN)
â”œâ”€â”€ requirements.txt              # Python Dependencies
â””â”€â”€ FinderEye.xcodeproj           # Xcode Project
```

---

## ğŸš€ Getting Started

### Prerequisites

| Requirement | Version |
|:------------|:--------|
| **Xcode** | 15.0+ |
| **iOS Device** | iOS 17.0+ (Camera requires a physical device) |
| **Python** | 3.12+ *(only for model export)* |

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/your-username/FinderEye.git
cd FinderEye

# 2. Open in Xcode
open FinderEye.xcodeproj
```

**Then in Xcode:**

1. Select `FinderEye` Target â†’ **Signing & Capabilities** â†’ choose your **Team**
2. Connect your iPhone and select it as the run destination
3. Build and Run (**âŒ˜ + R**)

> [!NOTE]
> The app will request camera permissions on first launch. Please allow it to enable real-time detection.

---

## ğŸ§  Model Management

FinderEye uses **YOLO-World** models exported to **Core ML** format. You can customize the detection vocabulary using the provided Python scripts.

### Setup & Export

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Export model (downloads base model if needed)
python3 Scripts/export_model.py
```

### Customize Vocabulary

Edit `Scripts/export_model.py` to modify the `base_objects` list:

```python
base_objects = [
    "person", "bicycle", "car", ..., "your_new_object"
]
```

> [!IMPORTANT]
> After adding new objects, update the Chinese mapping in `FinderEye/Sources/Core/Utils/ObjectTranslation.swift` to support Chinese search queries.

The export script will:
1. Download the base YOLO-World model (if needed)
2. Embed the custom vocabulary
3. Export to `FinderEye/Sources/Models/Resources/ObjectDetector.mlpackage`
4. Automatically replace the old model

---

## ğŸ—º Roadmap

- [x] Real-time OCR text search
- [x] Open-vocabulary object detection
- [x] Chinese natural language query support
- [x] Document perspective correction
- [ ] Multi-language query support
- [ ] Detection history & favorites
- [ ] iPad optimization
- [ ] Widget support

---

## ğŸ“ License

Copyright Â© 2026 FinderEye. All rights reserved.

